{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planet: Understanding the Amazon deforestation from Space challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to the kernel contributors of this challenge (especially @anokas and @Kaggoo) who helped me find a starting point for this notebook.\n",
    "\n",
    "The whole code including the `data_helper.py` and `keras_helper.py` files are available on github [here](https://github.com/EKami/planet-amazon-deforestation) and the notebook can be found on the same github [here](https://github.com/EKami/planet-amazon-deforestation/blob/master/notebooks/amazon_forest_notebook.ipynb)\n",
    "\n",
    "**If you found this notebook useful some upvotes would be greatly appreciated! :) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by adding the helper files to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import data_helper\n",
    "import keras_helper_ResNet50\n",
    "import keras_helper_VGG19\n",
    "import keras_helper_DenseNet121\n",
    "from keras_helper import AmazonKerasClassifier\n",
    "from keras_helper_ResNet50 import AmazonKerasClassifier_ResNet50\n",
    "from keras_helper_VGG19 import AmazonKerasClassifier_VGG19\n",
    "from keras_helper_DenseNet121 import AmazonKerasClassifier_DenseNet121\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from kaggle_data.downloader import KaggleDataDownloader\n",
    "from keras import backend\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print tensorflow version for reuse (the Keras module is used directly from the tensorflow framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_decal = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect image labels\n",
    "Visualize what the training set looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_jpeg_dir, test_jpeg_dir, test_jpeg_additional, train_csv_file = data_helper.get_jpeg_data_files_paths()\n",
    "train_jpeg_dir = \"/shared_datasets/kaggle/Amazon/data/train-jpg\"\n",
    "test_jpeg_dir = \"/shared_datasets/kaggle/Amazon/data/test-jpg\"\n",
    "test_jpeg_additional = \"/shared_datasets/kaggle/Amazon/data/test-jpg-additional\"\n",
    "train_csv_file = \"/shared_datasets/kaggle/Amazon/data/train_v2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters\n",
    "Define the hyperparameters of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_resize = (224, 224) # The resize size of each image\n",
    "validation_split_size = 0.2\n",
    "batch_size = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Preprocess the data in order to fit it into the Keras model.\n",
    "\n",
    "Due to the hudge amount of memory the resulting matrices will take, the preprocessing will be splitted into several steps:\n",
    "    - Preprocess training data (images and labels) and train the neural net with it\n",
    "    - Delete the training data and call the gc to free up memory\n",
    "    - Preprocess the first testing set\n",
    "    - Predict the first testing set labels\n",
    "    - Delete the first testing set\n",
    "    - Preprocess the second testing set\n",
    "    - Predict the second testing set labels and append them to the first testing set\n",
    "    - Delete the second testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, y_map = data_helper.preprocess_train_data(train_jpeg_dir, train_csv_file, img_resize)\n",
    "# Free up all available memory space after this heavy operation\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=validation_split_size, random_state=42)\n",
    "\n",
    "#del x_train, y_train\n",
    "#gc.collect()\n",
    "\"\"\"\n",
    "X_train, X_valid, Y_train, Y_valid = data_helper.decal(X_train, X_valid, Y_train, Y_valid, 5, nb_decal)\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a checkpoint saves the best model weights across all epochs in the training process. This ensures that we will always use only the best weights when making our predictions on the test set rather than using the default which takes the final score from the last epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights.best_ResNet50_no_val.hdf5\"\n",
    "proba_to_save=\"../proba_file_ResNet50_no_val.npy\"\n",
    "file_to_save=\"../submission_file_ResNet50_no_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model and begin training. \n",
    "\n",
    "Before starting the training process, you should first set a learning rate annealing optimization schedule by choosing a series of learning rates (learn_rates) with corresponding number of epochs for each (epochs_arr).\n",
    "\n",
    "Alternatively, if you just want to run one training session at a fixed learning rate and num epochs you can just input one entry for each of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = AmazonKerasClassifier_ResNet50(img_resize, len(y_map))\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "epochs_arr = [10, 10, 5]\n",
    "learn_rates = [0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for learn_rate, epochs in zip(learn_rates, epochs_arr):   \n",
    "    if i==0:\n",
    "        for layer in classifier.base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    if i > 0:\n",
    "        #X_train, X_valid, Y_train, Y_valid = data_helper.decal(X_train, X_valid, Y_train, Y_valid, 5, 1)\n",
    "        #gc.collect()\n",
    "        classifier.load_weights(filepath)\n",
    "        for layer in classifier.base_model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "    tmp_train_losses = classifier.train_model(x_train, y_train, learn_rate, epochs, \n",
    "                                                                           batch_size, validation_split_size=validation_split_size, \n",
    "                                                                           train_callbacks=[checkpoint])\n",
    " \n",
    "    \"\"\"\n",
    "    tmp_train_losses, tmp_val_losses, fbeta_score = classifier.train_model(X_train, X_valid, Y_train, Y_valid, learn_rate, epochs, \n",
    "                                                                           batch_size, validation_split_size=validation_split_size, \n",
    "                                                                           train_callbacks=[checkpoint])\n",
    "    \"\"\"\n",
    "    train_losses += tmp_train_losses\n",
    "    del tmp_train_losses\n",
    "    gc.collect()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should load back in the best weights that were automatically saved by ModelCheckpoint during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = AmazonKerasClassifier_ResNet50(img_resize, 17)\n",
    "classifier.load_weights(filepath)\n",
    "print(\"Weights loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we do not overfit by plotting the losses of the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching our predictions lets preprocess the test data and delete the old training data matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets launch the predictions on the additionnal dataset (updated on 05/05/2017 on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classifier = AmazonKerasClassifier_ResNet50(img_resize, 17)\n",
    "for i in range(3, 5):\n",
    "    filepath = \"weights.best_\" + str(i) + \".hdf5\"\n",
    "    x_train, y_train, y_map = data_helper.preprocess_train_data(train_jpeg_dir, train_csv_file, img_resize)\n",
    "    # Free up all available memory space after this heavy operation\n",
    "    gc.collect();\n",
    "    \n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=validation_split_size, random_state=42)\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    _, X_valid, _, Y_valid = data_helper.decal(X_train, X_valid, Y_train, Y_valid, 5, i)\n",
    "    del X_train, Y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    classifier.load_weights(filepath)\n",
    "    prediction_val = classifier.predict(X_valid)\n",
    "    thresholds = data_helper.optimise_f2_thresholds(Y_valid, prediction_val)\n",
    "    np.save(\"threshold\" + str(i) + \".npy\", thresholds)\n",
    "    print str(i) + \" done\"\n",
    "    del X_valid, Y_valid\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del X_train, Y_train\n",
    "#gc.collect()\n",
    "\n",
    "prediction_val = classifier.predict(X_valid)\n",
    "thresholds = data_helper.optimise_f2_thresholds(Y_valid, prediction_val)\n",
    "\n",
    "del X_valid, Y_valid, prediction_val\n",
    "gc.collect()\n",
    "\n",
    "#del X_train, Y_train, X_valid, Y_valid\n",
    "#gc.collect()\n",
    "#thresholds = [0.2] * 17#len(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test, x_test_filename = data_helper.preprocess_test_data(test_jpeg_dir, img_resize)\n",
    "predictions = classifier.predict_TTA(x_test)\n",
    "\n",
    "del x_test\n",
    "gc.collect()\n",
    "\n",
    "x_test, x_test_filename_additional = data_helper.preprocess_test_data(test_jpeg_additional, img_resize)\n",
    "new_predictions = classifier.predict_TTA(x_test)\n",
    "\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before mapping our predictions to their appropriate labels we need to figure out what threshold to take for each class.\n",
    "\n",
    "To do so we will take the median value of each classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets map our predictions to their tags and use the thresholds we just retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_map = {0: 'agriculture',\n",
    " 1: 'artisinal_mine',\n",
    " 2: 'bare_ground',\n",
    " 3: 'blooming',\n",
    " 4: 'blow_down',\n",
    " 5: 'clear',\n",
    " 6: 'cloudy',\n",
    " 7: 'conventional_mine',\n",
    " 8: 'cultivation',\n",
    " 9: 'habitation',\n",
    " 10: 'haze',\n",
    " 11: 'partly_cloudy',\n",
    " 12: 'primary',\n",
    " 13: 'road',\n",
    " 14: 'selective_logging',\n",
    " 15: 'slash_burn',\n",
    " 16: 'water'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictions_tot = np.load(\"/home/jb/amazon/predictions_tot.npy\")\n",
    "predictions_tot = np.vstack((predictions, new_predictions))\n",
    "np.save(proba_to_save, predictions_tot)\n",
    "predicted_labels = classifier.map_predictions_TTA(predictions_tot, y_map, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO complete\n",
    "tags_pred = np.array(predictions_tot).T\n",
    "_, axs = plt.subplots(5, 4, figsize=(15, 20))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, tag_vals in enumerate(tags_pred):\n",
    "    sns.boxplot(tag_vals, orient='v', palette='Set2', ax=axs[i]).set_title(y_map[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets assemble and visualize our prediction for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_list = [None] * len(predicted_labels)\n",
    "for i, tags in enumerate(predicted_labels):\n",
    "    tags_list[i] = ' '.join(map(str, tags))\n",
    "\n",
    "x_test_filename_tot = np.hstack((x_test_filename, x_test_filename_additional))\n",
    "final_data = [[filename.split(\".\")[0], tags] for filename, tags in zip(x_test_filename_tot, tags_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_data, columns=['image_name', 'tags'])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a lot of `primary` and `clear` tags, this final dataset may be legit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save it to a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(file_to_save, index=False)\n",
    "classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "del x_test\n",
    "gc.collect()\n",
    "\n",
    "x_test, x_test_filename_additional = data_helper.preprocess_test_data(test_jpeg_additional, img_resize)\n",
    "new_predictions = classifier.predict_TTA(x_test)\n",
    "#new_predictions = classifier.predict(x_test)\n",
    "\n",
    "del x_test\n",
    "gc.collect()\n",
    "predictions = np.vstack((predictions, new_predictions))\n",
    "x_test_filename = np.hstack((x_test_filename, x_test_filename_additional))\n",
    "print(\"Predictions shape: {}\\nFiles name shape: {}\\n1st predictions entry:\\n{}\".format(predictions.shape, \n",
    "                                                                              x_test_filename.shape,\n",
    "                                                                              predictions[0]))\n",
    "\n",
    "del x_test\n",
    "gc.collect()\n",
    "\n",
    "x_test, x_test_filename_additional = data_helper.preprocess_test_data(test_jpeg_additional, img_resize)\n",
    "x_test_filename = np.hstack((x_test_filename, x_test_filename_additional))\n",
    "\n",
    "predicted_labels = classifier.map_predictions(predictions, y_map, thresholds)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
